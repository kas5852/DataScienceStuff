{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’. Focus on pages 1-19 (up to Part II), the remaining part is more relevant for communication.\n",
    "http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "Summarize what you learned briefly (e.g. half a page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper talks about various communication systems and provides us with\n",
    "mathematical models on how take into account noise in the channel and interpret he\n",
    "original message from the final destination. It starts off with an introduction to the\n",
    "essential parts of a communication system: information source, transmitter, channel,\n",
    "receiver and destination. Part I mainly focuses on discrete noiseless systems such as\n",
    "teletype and telegraphy where initially we learn about the symbols used to transmit\n",
    "information and how to calculate the capacity of a channel. The channel capacity C is\n",
    "equal to $log_{W}$ where $W$ is the largest real root of the determinant equation:\n",
    "\n",
    "|$\\sum$$W^b_{ij}$ - $\\varphi$$_{ij}$ = 0\n",
    "\n",
    "\n",
    "We learn about stochastic processes and try many types using different criteria for\n",
    "transition probabilities and different orders of approximation (n-gram structures). This\n",
    "leads to a discussion about how Markoff and in particular the ‘ergodic’ processes\n",
    "represent a discrete source of information. Next we learn about the entropy (H) of a set\n",
    "of probabilities and the assumptions it must satisfy:\n",
    "\n",
    "$H$ = $-K$ $\\sum _{i=1} ^{n}$ $p_{i}logp_{i}$\n",
    "\n",
    "We consider a finite discrete source and the definition for entropy of a source as the\n",
    "weighted average of the individual entropies and find that H is thus approximately the\n",
    "logarithm of the reciprocal probability of a typical long sequence divided by the number\n",
    "of symbols in the sequence. We also get an understanding of what relative entropy and\n",
    "redundancy mean through real life examples. \n",
    "\n",
    "While understanding the operations performed by the transmitter and the receiver\n",
    "(transducers) we learn about non-singular and inverse transducers and the two\n",
    "functions they are described by. We then see how H can be interpreted as the rate of\n",
    "generating information by proving that H determined the channel capacity with the\n",
    "most efficient coding. Considering real life examples we learn how the entropy of a\n",
    "source determines the channel capacity which is necessary and sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import shutil\n",
    "import PyPDF2 \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np \n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Scraping, Entropy and ICML papers.\n",
    "\n",
    "ICML is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2018 papers\n",
    "from http://proceedings.mlr.press/v80/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PDFs downloaded!\n"
     ]
    }
   ],
   "source": [
    "# specify the URL of the archive here \n",
    "archive_url = \"http://proceedings.mlr.press/v80/\"\n",
    "  \n",
    "def get_pdf_links(): \n",
    "      \n",
    "    # create response object \n",
    "    r = requests.get(archive_url) \n",
    "      \n",
    "    # create beautiful-soup object \n",
    "    soup = BeautifulSoup(r.content) \n",
    "      \n",
    "    # find all links on web-page \n",
    "    links = soup.findAll('a',text=\"Download PDF\") \n",
    "    \n",
    "#     print(links)\n",
    "  \n",
    "    # filter the link sending with .mp4 \n",
    "    pdf_links = [ link['href'] for link in links if link['href'].endswith('pdf')] \n",
    "  \n",
    "    return pdf_links \n",
    "  \n",
    "  \n",
    "def download_pdf(pdf_links): \n",
    "  \n",
    "    for link in pdf_links: \n",
    "  \n",
    "        '''iterate through all links in video_links \n",
    "        and download them one by one'''\n",
    "          \n",
    "        # obtain filename by splitting url and getting  \n",
    "        # last string \n",
    "        file_name = link.split('/')[-1]  \n",
    "          \n",
    "        # create response object \n",
    "        r = requests.get(link, stream = True) \n",
    "         \n",
    "            \n",
    "        with open(\"C:\\\\Users\\\\Karim Sabar\\\\Desktop\\\\PDFs\\\\\"+file_name,\"wb\") as pdf:\n",
    "            shutil.copyfileobj(r.raw,pdf)\n",
    "#             for chunk in r.iter_content(chunk_size=1024): \n",
    "#                 if chunk: \n",
    "#                     pdf.write(chunk)\n",
    "          \n",
    "        \n",
    "  \n",
    "    print(\"All PDFs downloaded!\")\n",
    "    return\n",
    "  \n",
    "if __name__ == \"__main__\": \n",
    "  \n",
    "    pdf_links = get_pdf_links() \n",
    "  \n",
    "    download_pdf(pdf_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the top 10 common words in the ICML papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert( pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    text=\"\"\n",
    "    fileList= os.listdir('C:\\\\Users\\\\Karim Sabar\\\\Desktop\\\\PDFs2\\\\')\n",
    "    fileList.sort()\n",
    "    print(\"Starting Convert\")\n",
    "    \n",
    "    for filename in fileList:\n",
    "        if filename.endswith('.pdf'):\n",
    "            output = StringIO()\n",
    "            manager = PDFResourceManager()\n",
    "            converter = TextConverter(manager, output, laparams=LAParams())\n",
    "            interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "#             print(\"Starting: \"+filename)\n",
    "            infile = open(\"C:\\\\Users\\\\Karim Sabar\\\\Desktop\\\\PDFs2\\\\\"+filename, 'rb')\n",
    "            for page in PDFPage.get_pages(infile, pagenums):\n",
    "                interpreter.process_page(page)\n",
    "            infile.close()\n",
    "            converter.close()\n",
    "            text2 = output.getvalue()\n",
    "#             with open(\"./TXTs2/\"+filename+\".txt\",\"wb\") as txt:\n",
    "#                 txt.write(text2.encode())\n",
    "            \n",
    "            text=text+\" \"+text2\n",
    "            output.close\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Convert\n"
     ]
    }
   ],
   "source": [
    "#Used only used between 100-150 PDFs because it was taking wayyy too long \n",
    "if __name__ == \"__main__\": \n",
    "    tokens=[\"\"]\n",
    "    text=convert()\n",
    "    print(\"Done Converting\")\n",
    "    token = word_tokenize(text)\n",
    "    for word in token:\n",
    "        tokens.append(word)\n",
    "    punctuations = ['(',')',';',':','[',']',',','.','=','The']\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    keywords = [word for word in tokens if not word in stop_words and not word in punctuations and len(word)>2 and word.isalpha()]\n",
    "    \n",
    "    print(\"Starting to count\")\n",
    "    counter = Counter(keywords)\n",
    "    \n",
    "    most_occur = counter.most_common(10) \n",
    "    print(most_occur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let Z be a randomly selected word in a randomly selected ICML paper. Estimate the entropy\n",
    "of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    word_probs = []\n",
    "    unique_words = []\n",
    "    entrophy = 0\n",
    "    \n",
    "    for word in counter: \n",
    "        unique_words.append(word)\n",
    "        entrophy+= -1*(counter[word]/len(keywords))*(np.log2(counter[word]/len(keywords)))\n",
    "        word_probs.append(counter[word]/len(keywords))\n",
    "    \n",
    "    print(entrophy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Synthesize a random paragraph using the marginal distribution over words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range (80):\n",
    "        print(np.random.choice(unique_words, p=word_probs), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
